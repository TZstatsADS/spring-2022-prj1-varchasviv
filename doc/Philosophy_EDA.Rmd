---
title: "R Notebook"
output: html_notebook
---

```{r include = F}
knitr::opts_chunk$set(echo=F)
knitr::opts_chunk$set(warning=F)
knitr::opts_chunk$set(message=F)
```


```{r}
# Installing and Loading Required Packages

packages.used=c("rvest", "tibble", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "topicmodels", "stringr","plyr","knitr",
                "word2vec","ggwordcloud","gridExtra","grid","wordcloud")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
library("syuzhet")
library("sentimentr")
library("gplots")
library("plyr")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("stringr")
library("knitr")
library("word2vec")
library("ggwordcloud")
library("gridExtra")
library("grid")
library("wordcloud")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

```{r}
# Loading in the Data
sentence.list <- read.csv("../data/philosophy_data.csv")
```

```{r}
# Philosophy Books in the Schools of Communism and Capitalism

unique(sentence.list[,c('author','school')])[unique(sentence.list[,c('author','school')])$school %in% c("communism","capitalism"),]
```

```{r}
# Subsetting Data from Communism and Capitalism Books

commcap <- c("communism","capitalism")
sentence.list.commcap <- sentence.list %>% filter(school %in% commcap)

```



```{r}
# Adding Sentence ID - Index of the Sentence in its Corresponding Book

my.add.index <- function(df){
  df$sent.id <- 1:nrow(df)
  return(df)
}
sentence.list.commcap <- ddply(sentence.list.commcap,.(title),my.add.index)
```


```{r}
# Preparing for Corpus for Latent Dirichlet Allocation

corpus.list=sentence.list.commcap[2:(nrow(sentence.list.commcap)-1), ]
sentence.pre=sentence.list.commcap$sentence_str[1:(nrow(sentence.list.commcap)-2)]
sentence.post=sentence.list.commcap$sentence_str[3:(nrow(sentence.list.commcap)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentence_str, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text mining
```{r}
# Creating Corpus

docs <- Corpus(VectorSource(corpus.list$snipets))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r, warning = F, message=F}
# Data Cleaning to Avoid Triviality and Redundancy

set.seed(1)
index <- sample(1:nrow(corpus.list), 1)

print("Sample sentence:")
writeLines(as.character(docs[[index]]))

print("After converting to lower case:")
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[index]]))

print("After removing punctuation:")
#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[index]]))

print("After removing numbers:")
#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[index]]))

print("After removing stopwords:")
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[index]]))

print("After removing whitespace:")
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[index]]))

print("After stemming:")
#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[index]]))
```

### Topic modeling

Generate document-term matrices. 

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Creating Document Term Matrix (DTM) for LDA

dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$author, corpus.list$title,
                       corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals <- rep(NA, nrow(dtm))
for (i in 1:(nrow(dtm)%/%5000+1)){
  
  if(i!=(nrow(dtm)%/%5000+1)){
    rowTotals[(5000*(i-1)+1):(5000*i)] <- apply(dtm[(5000*(i-1)+1):(5000*i),],1,sum)
  }
  else{
    rowTotals[(5000*(i-1)+1):nrow(dtm)] <- apply(dtm[(5000*(i-1)+1):nrow(dtm),],1,sum)
  }
  
}

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

# Saving DTM-related Files
save(dtm, file = "../output/dtm")
save(corpus.list, file = "../output/corpus.list")
```

```{r}
# Loading in pre-saved DTM-related Files to Avoid Long Computation

load("../output/dtm")
load("../output/corpus.list")
```


Run LDA

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Running LDA and Saving Related Files for Future Use

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 8

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))

save(ldaOut, file = paste("../output/LDAGibbs",k, sep="_"))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
save(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics", sep="_"))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv", sep="_"))

#top 8 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,8))
save(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms", sep="_"))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv", sep="_"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
save(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities", sep="_"))
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv", sep="_"))
```

```{r}
# Loading in pre-saved LDA-related Files to Avoid Long Computation

load("../output/LDAGibbs_8_DocsToTopics")
load("../output/LDAGibbs_8_TopicsToTerms")
load("../output/LDAGibbs_8_TopicProbabilities")
```

```{r}
# Computing Most Salient and Most Popular Terms by Topic

terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
#t(topics.terms)
#ldaOut.terms

cat("The most popular and salient terms for each topic:\n\n")
kable(rbind(t(topics.terms),ldaOut.terms), format="simple", caption= "Table 1: The most popular and salient terms for each topic")
```

```{r}
# Preparing for Clustering

topics.hash=c("Production","Politics","Trade","Finance & Banking","Property","Assets","Socioeconomics","Time")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

## Clustering of topics
```{r, fig.width=8, fig.height=4}
par(mar=c(5.1, 4.1, 4.1, 2.1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(author, Production:Time)%>%
              group_by(author)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.summary <- topic.summary[c("Marx","Lenin","Keynes","Smith","Ricardo"),]

# [1] "Production"        "Politics"          "Trade"             "Finance & Banking" "Property"         
# [6] "Assets"            "Socioeconomics"    "Time"     

# 6, 7, 5, 1

topic.plot=c(1:8)
#print(topics.hash[topic.plot])
#topic.summary
heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 1, cexCol = 1, margins = c(4, 4),
          trace = "none", density.info = "none",
          main = "Heatmap of Topics by Author")
```

```{r}
communism <- sentence.list.commcap %>% filter(school == "communism")
all_communism_text <- paste(communism$sentence_str, collapse = " ")

set.seed(123456789)
model_communism <-word2vec(x =communism$sentence_str,type ="cbow",dim =25,iter =200)

embedding <-as.matrix(model_communism)
embedding <-predict(model_communism,c("money"),type ="embedding")
embedding

lookslike <-predict(model_communism,c("property"),type ="nearest",top_n =5)
lookslike
```

```{r}
capitalism <- sentence.list.commcap %>% filter(school == "capitalism")
all_capitalism_text <- paste(capitalism$sentence_str, collapse = " ")

set.seed(123456789)
model_capitalism <-word2vec(x =capitalism$sentence_str,type ="cbow",dim =25,iter =200)

embedding <-as.matrix(model_capitalism)
embedding <-predict(model_capitalism,c("money"),type ="embedding")
embedding

lookslike <-predict(model_capitalism,c("property"),type ="nearest",top_n =5)
lookslike
```

```{r, warning = F, message = F}
draw_word_cloud <- function(word){
  #par(mfrow=c(1,2), mar = c(0,0,0,0))
  lookslike_communism <-predict(model_communism,c(word),type ="nearest",top_n =20)
  lookslike_capitalism <-predict(model_capitalism,c(word),type ="nearest",top_n =20)
  
  plot1 <- ggplot(lookslike_communism[[1]], 
         aes(label = term2,
             size =  lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000,
             color=lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 15)+
    theme_minimal()+
    scale_color_gradient(low = "orangered", high = "darkred")+
    ggtitle("Communism")+
    theme(plot.title = element_text(hjust = 0.5))
  
  plot2 <- ggplot(lookslike_capitalism[[1]], 
         aes(label = term2,
             size =  lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000,
             color=lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 15)+
    theme_minimal()+
    scale_color_gradient(low = "lightblue", high = "darkblue")+
    ggtitle("Capitalism")+
    theme(plot.title = element_text(hjust = 0.5))
  

  arranged <- arrangeGrob(plot1, plot2, ncol=2, top = textGrob(paste("Words Used in Similar Contexts as '",word,"'", sep=""),gp=gpar(fontsize=20,font=1)), bottom = textGrob("Word size corresponds to similarity to the original word in the context of corresponding group.",gp=gpar(fontsize=10,font=1, alpha = 0.7)))
  
  plot(arranged)
  ggsave(file=paste("../figs/words_similar_to_",word,".jpg",sep=""), arranged, width=7.29, height=4.5)
}

draw_word_cloud("property")
draw_word_cloud("wages")
draw_word_cloud("taxes")

```


```{r}
lookslike <-predict(model_communism,c("taxes"),type ="nearest",top_n =5)
lookslike
lookslike <-predict(model_capitalism,c("taxes"),type ="nearest",top_n =5)
lookslike
```


```{r}
my_word <- "community"
lookslike <-predict(model_communism,my_word,type ="nearest",top_n =5)
lookslike
lookslike <-predict(model_capitalism,my_word,type ="nearest",top_n =5)
lookslike
```


```{r}
my_word <- "intervention"
lookslike <-predict(model_communism,my_word,type ="nearest",top_n =5)
lookslike
lookslike <-predict(model_capitalism,my_word,type ="nearest",top_n =5)
lookslike
```

