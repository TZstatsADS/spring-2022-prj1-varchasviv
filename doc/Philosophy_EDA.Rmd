---
title: "Communism vs Capitalism"
author: "Varchasvi Vedula"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r include = F}
knitr::opts_chunk$set(echo=F)
knitr::opts_chunk$set(warning=F)
knitr::opts_chunk$set(message=F)
knitr::opts_chunk$set(comment=NA)
```


```{r}
# Installing and Loading Required Packages

packages.used=c("rvest", "tibble", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "topicmodels", "stringr","plyr","knitr",
                "word2vec","ggwordcloud","gridExtra","grid","wordcloud",
                "tidytext","prettydoc", "DT")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
library("syuzhet")
library("sentimentr")
library("gplots")
library("plyr")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("stringr")
library("knitr")
library("word2vec")
library("ggwordcloud")
library("gridExtra")
library("grid")
library("wordcloud")
library("tidytext")
library("prettydoc")
library("DT")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

<center>
![Adam Smith and Karl Marx](logo.jpg){width=30%}
</center>

Throughout history, there have been developed many a political and socioeconomic framework to better the lives of those living within it. Of these, two schools have caught the attention of the world over the last few centuries, so much so that countries have gone to war over them - communism and capitalism. These frameworks are guided by the texts of those who formulated them over the years. These ideas are inherently philosophical, as they question the very meaning of society and what it means to be a part of it. To find out what these texts are talking about, let's analyze these philosophical text using the tools of Data Science and Natural Language Processing.

# Data Exploration

In this analysis, we use a data set of over 300,000 sentences from over 50 philosphical texts compiled by Kourosh Alizadeh for the Philosophy Data Project. We start by loading in the data set and exploring the shape and structure of the data set.

```{r}
# Loading in the Data
sentence.list <- read.csv("../data/philosophy_data.csv")
```

The dimensions of the data set are:



```{r}
dim(sentence.list)
```

The data set has the following columns:

```{r}
colnames(sentence.list)
```

We notice that the data is provided in a tidy format, so we directly proceed to filter the data that we are interested in. The following table shows the books in the schools of communism and capitalism included in this data set.

```{r}
# Philosophy Books in the Schools of Communism and Capitalism
commcap_books <- unique(sentence.list[,c('title','author','school')])[unique(sentence.list[,c('title','author','school')])$school %in% c("communism","capitalism"),]
rownames(commcap_books) <- NULL
datatable(commcap_books, rownames = FALSE, filter="top", options = list(pageLength = 6, scrollX=T) )
```

We've got 3 books from each school!

```{r}
# Subsetting Data from Communism and Capitalism Books

commcap <- c("communism","capitalism")
sentence.list.commcap <- sentence.list %>% filter(school %in% commcap)

```



```{r}
# Adding Sentence ID - Index of the Sentence in its Corresponding Book

my.add.index <- function(df){
  df$sent.id <- 1:nrow(df)
  return(df)
}
sentence.list.commcap <- ddply(sentence.list.commcap,.(title),my.add.index)
```

## Word Use Statistics

We are interested in seeing what is going on in these books. So, let's look at the kinds of words the communism books use versus those the capitalist books are using.

```{r}
communism <- sentence.list.commcap %>% filter(school == "communism")
all_communism_text <- paste(communism$sentence_str, collapse = " ")
capitalism <- sentence.list.commcap %>% filter(school == "capitalism")
all_capitalism_text <- paste(capitalism$sentence_str, collapse = " ")
```

```{r}
communism_text_df <- tibble(Text = all_communism_text) # tibble aka neater data frame
communism_text_words <- communism_text_df %>% unnest_tokens(output = word, input = Text) 
communism_text_words  <- communism_text_words  %>% anti_join(stop_words) # Remove stop words in peter_words
# Word Counts:
communism_text_wordcounts <- communism_text_words  %>% count(word, sort = TRUE)
communism_text_wordcounts_top_10 <- communism_text_wordcounts[1:10,]


capitalism_text_df <- tibble(Text = all_capitalism_text) # tibble aka neater data frame
capitalism_text_words <- capitalism_text_df %>% unnest_tokens(output = word, input = Text) 
capitalism_text_words  <- capitalism_text_words  %>% anti_join(stop_words) # Remove stop words in peter_words
# Word Counts:
capitalism_text_wordcounts <- capitalism_text_words  %>% count(word, sort = TRUE)
capitalism_text_wordcounts_top_10 <- capitalism_text_wordcounts[1:10,]
```

```{r}
freq_plot_communism <- ggplot(communism_text_wordcounts_top_10, aes(x = reorder(word, -n), y = n)) + geom_bar(stat = "identity",fill="darkred") + xlab("Word") + ylab("Frequency") + ggtitle("Top 10 Most Frequent Words in Books on Communism")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylim(c(0,3500))
freq_plot_capitalism <- ggplot(capitalism_text_wordcounts_top_10, aes(x = reorder(word, -n), y = n)) + geom_bar(stat = "identity",fill="darkblue")+ xlab("Word") + ylab("Frequency") + ggtitle("Top 10 Most Frequent Words in Books on Capitalism")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylim(c(0,3500))
grid.arrange(freq_plot_communism, freq_plot_capitalism, ncol=2)
```


LDA
```{r}
# Preparing for Corpus for Latent Dirichlet Allocation

corpus.list=sentence.list.commcap[2:(nrow(sentence.list.commcap)-1), ]
sentence.pre=sentence.list.commcap$sentence_str[1:(nrow(sentence.list.commcap)-2)]
sentence.post=sentence.list.commcap$sentence_str[3:(nrow(sentence.list.commcap)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentence_str, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text mining
```{r}
# Creating Corpus

docs <- Corpus(VectorSource(corpus.list$snipets))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r, warning = F, message=F}
# Data Cleaning to Avoid Triviality and Redundancy

set.seed(1)
index <- sample(1:nrow(corpus.list), 1)

print("Sample sentence:")
writeLines(as.character(docs[[index]]))

print("After converting to lower case:")
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[index]]))

print("After removing punctuation:")
#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[index]]))

print("After removing numbers:")
#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[index]]))

print("After removing stopwords:")
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[index]]))

print("After removing whitespace:")
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[index]]))

print("After stemming:")
#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[index]]))
```

### Topic modeling

Generate document-term matrices. 

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Creating Document Term Matrix (DTM) for LDA

dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$author, corpus.list$title,
                       corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals <- rep(NA, nrow(dtm))
for (i in 1:(nrow(dtm)%/%5000+1)){
  
  if(i!=(nrow(dtm)%/%5000+1)){
    rowTotals[(5000*(i-1)+1):(5000*i)] <- apply(dtm[(5000*(i-1)+1):(5000*i),],1,sum)
  }
  else{
    rowTotals[(5000*(i-1)+1):nrow(dtm)] <- apply(dtm[(5000*(i-1)+1):nrow(dtm),],1,sum)
  }
  
}

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

# Saving DTM-related Files
save(dtm, file = "../output/dtm")
save(corpus.list, file = "../output/corpus.list")
```

```{r}
# Loading in pre-saved DTM-related Files to Avoid Long Computation

load("../output/dtm")
load("../output/corpus.list")
```


Run LDA

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Running LDA and Saving Related Files for Future Use

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 8

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))

save(ldaOut, file = paste("../output/LDAGibbs",k, sep="_"))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
save(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics", sep="_"))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv", sep="_"))

#top 8 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,8))
save(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms", sep="_"))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv", sep="_"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
save(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities", sep="_"))
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv", sep="_"))
```

```{r}
# Loading in pre-saved LDA-related Files to Avoid Long Computation

load("../output/ldaGibbs_8")
load("../output/LDAGibbs_8_DocsToTopics")
load("../output/LDAGibbs_8_TopicsToTerms")
load("../output/LDAGibbs_8_TopicProbabilities")
```

```{r}
# Computing Most Salient and Most Popular Terms by Topic

#Number of topics
k <- 8

terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
#t(topics.terms)
#ldaOut.terms

cat("The most popular and salient terms for each topic:\n\n")
kable(rbind(t(topics.terms),ldaOut.terms), format="simple", caption= "Table 1: The most popular and salient terms for each topic")
```

```{r}
# Preparing for Clustering

topics.hash=c("Production","Politics","Trade","Finance & Banking","Property","Assets","Socioeconomics","Time")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

## Clustering of topics
```{r, fig.width=8, fig.height=4, message = F, warning = F}
par(mar=c(5.1, 4.1, 4.1, 2.1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(author, Production:Time)%>%
              group_by(author)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.summary <- topic.summary[c("Marx","Lenin","Keynes","Smith","Ricardo"),]

# [1] "Production"        "Politics"          "Trade"             "Finance & Banking" "Property"         
# [6] "Assets"            "Socioeconomics"    "Time"     

# 6, 7, 5, 1

topic.plot=c(1:8)
#print(topics.hash[topic.plot])
#topic.summary

# JPEG device
jpeg(file = "../output/heatmap_author_by_topic.jpeg")

# Code
heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 1, cexCol = 1, margins = c(8,8),
          trace = "none", density.info = "none",
          main = "Heatmap of Topics by Author")

# Close device
trash <- dev.off()

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 1, cexCol = 1, margins = c(8,8),
          trace = "none", density.info = "none",
          main = "Heatmap of Topics by Author")

```


```{r, eval = F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

set.seed(5)
model_communism <-word2vec(x =communism$sentence_lowered,type ="cbow",dim =20,iter =200)

write.word2vec(model_communism,"../output/word2vec_communism.bin")

set.seed(5)
model_capitalism <-word2vec(x =capitalism$sentence_lowered,type ="cbow",dim =20,iter =200)

write.word2vec(model_capitalism,"../output/word2vec_capitalism.bin")
```

```{r}
model_communism <-read.word2vec("../output/word2vec_communism.bin")
model_capitalism <-read.word2vec("../output/word2vec_capitalism.bin")
```


```{r, warning = F, message = F}
draw_word_cloud <- function(word){
  #par(mfrow=c(1,2), mar = c(0,0,0,0))
  lookslike_communism <-predict(model_communism,c(word),type ="nearest",top_n =20)
  lookslike_capitalism <-predict(model_capitalism,c(word),type ="nearest",top_n =20)
  
  plot1 <- ggplot(lookslike_communism[[1]], 
         aes(label = term2,
             size =  lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000,
             color=lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 10)+
    theme_minimal()+
    scale_color_gradient(low = "orangered", high = "darkred")+
    ggtitle("Communism")+
    theme(plot.title = element_text(hjust = 0.5))
  
  plot2 <- ggplot(lookslike_capitalism[[1]], 
         aes(label = term2,
             size =  lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000,
             color=lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 10)+
    theme_minimal()+
    scale_color_gradient(low = "lightblue", high = "darkblue")+
    ggtitle("Capitalism")+
    theme(plot.title = element_text(hjust = 0.5))
  

  arranged <- arrangeGrob(plot1, plot2, ncol=2, top = textGrob(paste("Words Used in Similar Contexts as '",word,"'", sep=""),
                                                               gp=gpar(fontsize=20,font=1)), 
                          bottom = textGrob("Word size corresponds to similarity to the original word in the context of corresponding group.",
                                            gp=gpar(fontsize=10,font=1, alpha = 0.7)))
  
  plot(arranged)
  ggsave(file=paste("../figs/words_similar_to_",word,".jpg",sep=""), arranged, width=7.29, height=4.5)
}

draw_word_cloud("property")
draw_word_cloud("wages")
draw_word_cloud("production")
draw_word_cloud("taxes")
draw_word_cloud("poor")
draw_word_cloud("commerce")
```




```{r}
my_word <- "community"
lookslike <-predict(model_communism,my_word,type ="nearest",top_n =5)
lookslike
lookslike <-predict(model_capitalism,my_word,type ="nearest",top_n =5)
lookslike
```


```{r}
my_word <- "intervention"
lookslike <-predict(model_communism,my_word,type ="nearest",top_n =5)
lookslike
lookslike <-predict(model_capitalism,my_word,type ="nearest",top_n =5)
lookslike
```

