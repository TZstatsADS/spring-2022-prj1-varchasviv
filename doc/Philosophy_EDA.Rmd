---
title: "R Notebook"
output: html_notebook
---

```{r include = F}
knitr::opts_chunk$set(echo=F, message=F, warning=F)
```


```{r}
packages.used=c("rvest", "tibble", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "topicmodels", "stringr","plyr","knitr")
# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
library("syuzhet")
library("sentimentr")
library("gplots")
library("plyr")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("stringr")
library("knitr")

source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

```{r}
sentence.list <- read.csv("../data/philosophy_data.csv")
```

```{r}
sort(unique(sentence.list$author))

unique(sentence.list$title[sentence.list$author=="Kant"])

unique(sentence.list[,c('author','school')])[unique(sentence.list[,c('author','school')])$school %in% c("communism","capitalism"),]
```

```{r}
commcap <- c("communism","capitalism")
commcap <- c("empiricism","rationalism")

sentence.list.commcap <- sentence.list %>% filter(school %in% commcap)

```



```{r}
my.add.index <- function(df){
  df$sent.id <- 1:nrow(df)
  return(df)
}
sentence.list.commcap <- ddply(sentence.list.commcap,.(title),my.add.index)
```


```{r}
corpus.list=sentence.list.commcap[2:(nrow(sentence.list.commcap)-1), ]
sentence.pre=sentence.list.commcap$sentence_str[1:(nrow(sentence.list.commcap)-2)]
sentence.post=sentence.list.commcap$sentence_str[3:(nrow(sentence.list.commcap)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentence_str, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text mining
```{r}
docs <- Corpus(VectorSource(corpus.list$snipets))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r, warning = F, message=F}
set.seed(1)
index <- sample(1:nrow(corpus.list), 1)

print("Sample sentence:")
writeLines(as.character(docs[[index]]))

print("After converting to lower case:")
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[index]]))

print("After removing punctuation:")
#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[index]]))

print("After removing numbers:")
#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[index]]))

print("After removing stopwords:")
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[index]]))

print("After removing whitespace:")
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[index]]))

print("After stemming:")
#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[index]]))
```

### Topic modeling

Generate document-term matrices. 

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$author, corpus.list$title,
                       corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals <- rep(NA, nrow(dtm))
for (i in 1:(nrow(dtm)%/%5000+1)){
  
  if(i!=(nrow(dtm)%/%5000+1)){
    rowTotals[(5000*(i-1)+1):(5000*i)] <- apply(dtm[(5000*(i-1)+1):(5000*i),],1,sum)
  }
  else{
    rowTotals[(5000*(i-1)+1):nrow(dtm)] <- apply(dtm[(5000*(i-1)+1):nrow(dtm),],1,sum)
  }
  
}

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

save(dtm, file = "../output/dtm")
save(corpus.list, file = "../output/corpus.list")
```

```{r}
load("../output/dtm")
load("../output/corpus.list")
```


Run LDA

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 8

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))

save(ldaOut, file = paste("../output/LDAGibbs",k, sep="_"))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
save(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics", sep="_"))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv", sep="_"))

#top 8 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,8))
save(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms", sep="_"))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv", sep="_"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
save(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities", sep="_"))
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv", sep="_"))
```

```{r}
load("../output/LDAGibbs_8_DocsToTopics")
load("../output/LDAGibbs_8_TopicsToTerms")
load("../output/LDAGibbs_8_TopicProbabilities")
```

```{r}
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
#t(topics.terms)
#ldaOut.terms

cat("The most popular and salient terms for each topic:\n\n")
kable(rbind(t(topics.terms),ldaOut.terms), format="simple", label= "The most popular and salient terms for each topic")
```

```{r}
topics.hash=c("Production","Politics","Trade","Finance & Banking","Property","Assets","Socioeconomics","Time")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

## Clustering of topics
```{r, fig.width=8, fig.height=4}
par(mar=c(5.1, 4.1, 4.1, 2.1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(author, Production:Time)%>%
              group_by(author)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.summary <- topic.summary[c("Marx","Lenin","Keynes","Smith","Ricardo"),]

# [1] "Production"        "Politics"          "Trade"             "Finance & Banking" "Property"         
# [6] "Assets"            "Socioeconomics"    "Time"     

# 6, 7, 5, 1

topic.plot=c(1:8)
#print(topics.hash[topic.plot])
#topic.summary
heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = bluered(100),
          cexRow = 1, cexCol = 1, margins = c(4, 4),
          trace = "none", density.info = "none",
          main = "Heatmap of Topics by Author")
```
```{r, fig.width=3.3, fig.height=5, eval = F}
# [1] "Production"        "Politics"          "Trade"             "Finance & Banking" "Property"         
# [6] "Assets"            "Socioeconomics"    "Time"     
 

par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")

topic.plot=c(6, 7, 5, 1)
print(topics.hash[topic.plot])

speech.df=tbl_df(corpus.list.df)%>%filter(author=="Marx", title == "The Communist Manifesto")%>%select(sent.id, Production:Time)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1], 
             xlab="Sentences", ylab="Topic share", main="The Communist Manifesto, Karl Marx")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="WilliamJClinton", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Bill Clinton, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeWBush", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1], 
            xlab="Sentences", ylab="Topic share", main="George W Bush, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="BarackObama", type=="nomin", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Barack Obama, Nomination")

speech.df=tbl_df(corpus.list.df)%>%filter(File=="DonaldJTrump", type=="nomin")%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
            xlab="Sentences", ylab="Topic share", main="Donald Trump, Nomination")
```


Mind and body. Cartesian - duality between mind and body. justice, good, mind. male vs female, feminism - wollstonecraft. naturalism. religion, god. Deontolgy and consequentialsm. location. free will. society vs self. females focused more on society

good
harm
action