---
title: "The Cold War"
author: "Varchasvi Vedula"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

```{r include = F}
knitr::opts_chunk$set(echo=F)
knitr::opts_chunk$set(warning=F)
knitr::opts_chunk$set(message=F)
knitr::opts_chunk$set(comment=NA)
```


```{r}
# Installing and Loading Required Packages

packages.used=c("rvest", "tibble", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "topicmodels", "stringr","plyr","knitr",
                "word2vec","ggwordcloud","gridExtra","grid","wordcloud",
                "tidytext","prettydoc", "DT")
# Check for packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
library("syuzhet")
library("sentimentr")
library("gplots")
library("plyr")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("stringr")
library("knitr")
library("word2vec")
library("ggwordcloud")
library("gridExtra")
library("grid")
library("wordcloud")
library("tidytext")
library("prettydoc")
library("DT")


source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
```

<center>
![Adam Smith and Karl Marx](logo.jpg){width=30%}
</center>

The term "Cold War" was first used by writer George Orwell to describe the tensions that developed after World War II between the United States and the Soviet Union. Tensions built up on all fronts and the two superpowers were on the brink of a nuclear disaster. But what really drove these behemoths to this state?

On the forefront of these issues were ideological differences. The political and economic systems of the capitalistic USA and the communist Soviet Union were incompatible. But what really was so different between these two philosophies? Let's find out.


The frameworks of communism and capitalism are guided by the texts of those who formulated them over the centuries. These have authors have proved that the pen is truly mightier than the sword as they penned the very blueprints of the societies we know and live in today. These socioeconomic frameworks are inherently philosophical, as they question the very meaning of society and what it means to be a part of it. To find out what they texts are talking about, let’s analyze these philosophical texts using the tools of Data Science and Natural Language Processing. We are interested in looking at the vernacular of these schools of thought, what topics they primarily discuss, and how they relate to certain terms. 


*Disclaimer*: Since Communism and Capitalism are such ubiquitous topics in real life, we are aware of some of the principles of the two regimes. This may be a potential bias in our analysis. However, for the integrity of the data-driven study, we will only claim what we see through data and acknowledge any contextual opinions we may form along the way (prefaced by “Opinion:”). At the end, we will draw comparisons between what we found and what we know. Now, let's go.


(Depicted in the figure above: Adam Smith (Capitalist) and Karl Marx (Communist))


# Data Exploration

In this analysis, we use a data set of over 300,000 sentences from over 50 philosophical texts compiled by Kourosh Alizadeh for the Philosophy Data Project. We start by loading in the data set and exploring the shape and structure of the data set.

```{r}
# Loading in the Data
sentence.list <- read.csv("../data/philosophy_data.csv")
```

The dimensions of the data set are:



```{r}
dim(sentence.list)
```

The data set has the following columns:

```{r}
colnames(sentence.list)
```

We notice that the data is provided in a tidy format, so we directly proceed to filter the data that we are interested in. The following table shows the books in the schools of communism and capitalism included in this data set.

```{r}
# Philosophy Books in the Schools of Communism and Capitalism
commcap_books <- unique(sentence.list[,c('title','author','school')])[unique(sentence.list[,c('title','author','school')])$school %in% c("communism","capitalism"),]
rownames(commcap_books) <- NULL
datatable(commcap_books, rownames = FALSE, filter="top", options = list(pageLength = 6, scrollX=T) )
```

We've got 3 books from each school! Our communism authors are Marx and Lenin, while our capitalism authors are Smith, Keynes, and Ricardo.

```{r}
# Subsetting Data from Communism and Capitalism Books

commcap <- c("communism","capitalism")
sentence.list.commcap <- sentence.list %>% filter(school %in% commcap)

```



```{r}
# Adding Sentence ID - Index of the Sentence in its Corresponding Book

my.add.index <- function(df){
  df$sent.id <- 1:nrow(df)
  return(df)
}
sentence.list.commcap <- ddply(sentence.list.commcap,.(title),my.add.index)
```

## Word Frequency Analysis

We are interested in seeing what is going on in these books. So, let's look at the kinds of words the communism books use versus those the capitalist books are using.

```{r}
communism <- sentence.list.commcap %>% filter(school == "communism")
all_communism_text <- paste(communism$sentence_str, collapse = " ")
capitalism <- sentence.list.commcap %>% filter(school == "capitalism")
all_capitalism_text <- paste(capitalism$sentence_str, collapse = " ")
```

```{r}
communism_text_df <- tibble(Text = all_communism_text) # tibble aka neater data frame
communism_text_words <- communism_text_df %>% unnest_tokens(output = word, input = Text) 
communism_text_words  <- communism_text_words  %>% anti_join(stop_words) # Remove stop words in peter_words
# Word Counts:
communism_text_wordcounts <- communism_text_words  %>% count(word, sort = TRUE)
communism_text_wordcounts_top_10 <- communism_text_wordcounts[1:10,]


capitalism_text_df <- tibble(Text = all_capitalism_text) # tibble aka neater data frame
capitalism_text_words <- capitalism_text_df %>% unnest_tokens(output = word, input = Text) 
capitalism_text_words  <- capitalism_text_words  %>% anti_join(stop_words) # Remove stop words in peter_words
# Word Counts:
capitalism_text_wordcounts <- capitalism_text_words  %>% count(word, sort = TRUE)
capitalism_text_wordcounts_top_10 <- capitalism_text_wordcounts[1:10,]
```

```{r}
freq_plot_communism <- ggplot(communism_text_wordcounts_top_10, aes(x = reorder(word, -n), y = n)) + geom_bar(stat = "identity",fill="darkred") + xlab("Word") + ylab("Frequency") + ggtitle("Top 10 Most Frequent Words in Communism")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylim(c(0,3500)) + theme(plot.title = element_text(size = 10.5))
freq_plot_capitalism <- ggplot(capitalism_text_wordcounts_top_10, aes(x = reorder(word, -n), y = n)) + geom_bar(stat = "identity",fill="darkblue")+ xlab("Word") + ylab("Frequency") + ggtitle("Top 10 Most Frequent Words in Capitalism")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + ylim(c(0,3500))+ theme(plot.title = element_text(size = 11))
freq_arranged <- grid.arrange(freq_plot_communism, freq_plot_capitalism, ncol=2)

ggsave("../figs/frequency_plot_communism.jpg", width = 7.29, height = 4.5, plot = freq_plot_communism)
ggsave("../figs/frequency_plot_capitalism.jpg", width = 7.29, height = 4.5, plot = freq_plot_capitalism)
```

The plot above shows the top 10 most common words in books on communism and capitalism respectively. Both schools seem to use words related to money and manufacturing. However, communism includes a few words related to society while capitalism includes words related to the financial market. While it's hard to infer much from 10 words alone, this gives us a preliminary idea of what to expect from a deeper dive.

# Topic Modeling

Let's check out what some of the topics these books discuss are! To do this, we use a statistical model called Latent Dirichlet Allocation. This model attempts to capture the themes discussed across these texts by looking at the co-occurrence of words within and across texts. We try to find 8 latent topics from these texts.

We must note that LDA is a bag-of-words model, meaning that it only looks at the occurrence of words, not their order. This means that sentence structure is not modeled, leading to potentially missing contextual information. Moreover, we must manually choose how many topics to look for, which may not be equal to the actual number of topics there are in the data set. These are potential biases we must be aware of.

```{r}
# Preparing for Corpus for Latent Dirichlet Allocation

corpus.list=sentence.list.commcap[2:(nrow(sentence.list.commcap)-1), ]
sentence.pre=sentence.list.commcap$sentence_str[1:(nrow(sentence.list.commcap)-2)]
sentence.post=sentence.list.commcap$sentence_str[3:(nrow(sentence.list.commcap)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentence_str, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]
```

## Text Pre-Processing

Before we run the text through this model, we must process it first to avoid making trivial or incorrect conclusions. We lower the case of the text (to avoid confusion between "Labour" and "labour), remove punctuation (to avoid confusion between "labour." and "labour"), remove numbers (such as "9" from "9th"), remove stopwords that don't contribute to the meaning of a sentence, remove white space (such as "..tonight.    From now on,..."), and perform stemming to unify words with the same root (such as "labour" and "labouring").

While it may seem excessive, all of this is done in order to extract the most meaning out of our data.

```{r, warning = F, message=F}
# Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

# Creating Corpus
docs <- Corpus(VectorSource(corpus.list$snipets))

# Data Cleaning to Avoid Triviality and Redundancy

set.seed(2)
index <- sample(1:nrow(corpus.list), 1)

print("Sample sentence:")
writeLines(as.character(docs[[index]]))

print("After converting to lower case:")
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
writeLines(as.character(docs[[index]]))

print("After removing punctuation:")
#remove punctuation
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[[index]]))

print("After removing numbers:")
#Strip digits
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[[index]]))

print("After removing stopwords:")
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
writeLines(as.character(docs[[index]]))

print("After removing whitespace:")
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[index]]))

print("After stemming:")
#Stem document
docs <- tm_map(docs,stemDocument)
writeLines(as.character(docs[[index]]))
```

### LDA

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Creating Document Term Matrix (DTM) for LDA

dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$author, corpus.list$title,
                       corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals <- rep(NA, nrow(dtm))
for (i in 1:(nrow(dtm)%/%5000+1)){
  
  if(i!=(nrow(dtm)%/%5000+1)){
    rowTotals[(5000*(i-1)+1):(5000*i)] <- apply(dtm[(5000*(i-1)+1):(5000*i),],1,sum)
  }
  else{
    rowTotals[(5000*(i-1)+1):nrow(dtm)] <- apply(dtm[(5000*(i-1)+1):nrow(dtm),],1,sum)
  }
  
}

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

# Saving DTM-related Files
save(dtm, file = "../output/dtm")
save(corpus.list, file = "../output/corpus.list")
```

```{r}
# Loading in pre-saved DTM-related Files to Avoid Long Computation

load("../output/dtm")
load("../output/corpus.list")
```

```{r, eval=F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

# Running LDA and Saving Related Files for Future Use

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 8

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))

save(ldaOut, file = paste("../output/LDAGibbs",k, sep="_"))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
save(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics", sep="_"))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv", sep="_"))

#top 8 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,8))
save(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms", sep="_"))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv", sep="_"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
save(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities", sep="_"))
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv", sep="_"))
```

```{r}
# Loading in pre-saved LDA-related Files to Avoid Long Computation

load("../output/ldaGibbs_8")
load("../output/LDAGibbs_8_DocsToTopics")
load("../output/LDAGibbs_8_TopicsToTerms")
load("../output/LDAGibbs_8_TopicProbabilities")
```

The model identifies the 8 following unlabeled topics across both schools. For each topic, we list a combination of the most popular and salient terms. Looking at these terms, I manually assigned labels for each topic (shown in the table below)

```{r}
# Computing Most Salient and Most Popular Terms by Topic

#Number of topics
k <- 8

terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
#t(topics.terms)
#ldaOut.terms
display_terms <- rbind(t(topics.terms),ldaOut.terms)
colnames(display_terms) <- paste(paste(rep("Topic", 8), 1:8, rep(":", 8)), c("Production","Politics","Trade","Finance & Banking","Property","Assets","Socioeconomics","Time"), sep="")

datatable(display_terms, rownames = FALSE, filter="top", options = list(pageLength = 15, scrollX=T) )
```

Now that we know the main topics discussed in these books, let's find out which author talks about which topic the most. We use a technique called clustering.

```{r}
# Preparing for Clustering

topics.hash=c("Production","Politics","Trade","Finance & Banking","Property","Assets","Socioeconomics","Time")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

## Clustering of topics
```{r, fig.width=8, fig.height=4, message = F, warning = F}
par(mar=c(5.1, 4.1, 4.1, 2.1))
topic.summary=tbl_df(corpus.list.df)%>%
              select(author, Production:Time)%>%
              group_by(author)%>%
              summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.summary <- topic.summary[c("Marx","Lenin","Keynes","Smith","Ricardo"),]

# [1] "Production"        "Politics"          "Trade"             "Finance & Banking" "Property"         
# [6] "Assets"            "Socioeconomics"    "Time"     

# 6, 7, 5, 1

topic.plot=c(1:8)
#print(topics.hash[topic.plot])
#topic.summary

# JPEG device

jpeg(file = "../figs/heatmap_author_by_topic.jpeg")

# Code
heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = colorpanel(100, "white", "lightgreen", "darkgreen"),
          cexRow = 1, cexCol = 1, margins = c(8,8),
          trace = "none", density.info = "none",
          main = "Heatmap - Darker is Stronger")

# Close device
trash <- dev.off()

heatmap.2(as.matrix(topic.summary[,topic.plot+1]), 
          scale = "column", key=F, 
          col = colorpanel(100, low="white",  high="darkgreen"),
          cexRow = 1, cexCol = 1, margins = c(8,8),
          trace = "none", density.info = "none",
          main = "Heatmap of Topics-Darker is Stronger")

```

The plot above gives us a very interesting story, building up on the frequency plot. It shows us the proportion of their text that each auther spends on each topic. A darker rectangle implies that that author spends more time talking about that topic than a lighter rectangle. 

We notice that the communists (Marx and Lenin) spend more of their texts covering the topics of Production and Socioeconomics. On the other hand, the capitalists spend more of their texts covering more financially aggressive topics like Trade, Assets, Property, and Finance & Banking.

It seems like communism focuses more on the people and their welfare (Time, Socioeconomics) alongside their economy (Production). The lack of discussion of Assets indicates potentially less involvement of the people in the exchange of Assets. Similarly, the lack of discussion of Politics suggests that it may be exclusive and not open to the public. We noticed that the topic of Politics also discusses class, further suggesting that class structure may not exist under communism.

In addition to financial topics, capitalism also discusses politics, unlike communism, suggesting that there may be more of an opportunity for the people to be involved in the government. The abundance of discussion of Assets and Property suggests a more open market under this regime.

At this stage, all we know if whether or not certain authors discuss certain topics - we do not know if they are inclined one way or another. We are purely making an educated guess. To understand whether or not the authors are pro or anti the topics they talk about, we will conduct the following analysis.

# Similar Word Analysis

To fix the shortcomings of our previous step, we take the following approach. We create a model that takes a word as an input and returns other words used most similarly to our original word in the data set. This is helpful for two reasons:

1. We get to infer the authors' opinions on the word

2. We get to compare these opinions Communism vs Capitalism (over the same word)

To achieve this, we use a model called word2vec. Word2vec essentially assigns a set of numbers to each word in the data set. Words that are closer (have similar numbers) are used in similar contexts. We will train separate word2vec models for communism texts and capitalism texts.

We will use word clouds to represent word most similar to our given word. The bigger the word, the more similar it is to our original word.

One drawback of this method is that the similar words might not really make much sense in context of the original word. This may occur because the algorithm did not converge.

```{r, eval = F}
# WARNING: TIME INTENSIVE CHUNK. DO NOT RUN THIS CHUNK FOR EVALUATION
# THE REQUIRED OUTPUTS ARE SAVED IN OUTPUT FOLDER FOR EASY ACCESS.

set.seed(5)
model_communism <-word2vec(x =communism$sentence_lowered,type ="cbow",dim =20,iter =200)

write.word2vec(model_communism,"../output/word2vec_communism.bin")

set.seed(5)
model_capitalism <-word2vec(x =capitalism$sentence_lowered,type ="cbow",dim =20,iter =200)

write.word2vec(model_capitalism,"../output/word2vec_capitalism.bin")
```

```{r}
model_communism <-read.word2vec("../output/word2vec_communism.bin")
model_capitalism <-read.word2vec("../output/word2vec_capitalism.bin")
```


```{r, warning = F, message = F}
draw_word_cloud <- function(word){
  #par(mfrow=c(1,2), mar = c(0,0,0,0))
  lookslike_communism <-predict(model_communism,c(word),type ="nearest",top_n =20)
  lookslike_capitalism <-predict(model_capitalism,c(word),type ="nearest",top_n =20)
  
  plot1 <- ggplot(lookslike_communism[[1]], 
         aes(label = term2,
             size =  lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000,
             color=lookslike_communism[[1]]$similarity*10000-min(lookslike_communism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 10)+
    theme_minimal()+
    scale_color_gradient(low = "orangered", high = "darkred")+
    ggtitle("Communism")+
    theme(plot.title = element_text(hjust = 0.5))
  
  plot2 <- ggplot(lookslike_capitalism[[1]], 
         aes(label = term2,
             size =  lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000,
             color=lookslike_capitalism[[1]]$similarity*10000-min(lookslike_capitalism[[1]]$similarity)*10000)) +
    geom_text_wordcloud_area() +
    scale_size_area(max_size = 10)+
    theme_minimal()+
    scale_color_gradient(low = "lightblue", high = "darkblue")+
    ggtitle("Capitalism")+
    theme(plot.title = element_text(hjust = 0.5))
  

  arranged <- arrangeGrob(plot1, plot2, ncol=2, top = textGrob(paste("Words Used in Similar Contexts as '",word,"'", sep=""),
                                                               gp=gpar(fontsize=20,font=1)), 
                          bottom = textGrob("Word size corresponds to similarity to the original word in the context of corresponding group.",
                                            gp=gpar(fontsize=10,font=1, alpha = 0.7)))
  
  plot(arranged)
  ggsave(file=paste("../figs/words_similar_to_",word,".jpg",sep=""), arranged, width=7.29, height=4.5)
}
```

```{r}
draw_word_cloud("property")
```

Since we noticed that property was discussed in particularly different extents between the two schools, let's start with that. Immediately, we see that some of our suspicions were true. Based on the texts from communism, some of the most related words to "property" are "community", "common", and "society". 

Opinion: This may imply that property is be communal, owned by the state.

On the other hand, we see that under capitalism, "property" is similar to "persons", "revenue", and "class", suggesting individual ownership of property (and "revenue" suggesting renting property out for profit).

```{r}
draw_word_cloud("wages")
```

Looking at the text from communism, we see a typical set of words you would expect to be similar to "wages" - a "weekly" "rate", likely stabilized by the government for the whole community's welfare. Under capitalism, we see an emphasis on "profits", suggesting opportunity for business and an individual's own welfare.

```{r}
draw_word_cloud("production")
```

The word "production" does not seem to have very sensible similar words. This might be a drawback of the word2vec model. Under Capitalism, however, there seems to be an indication that "production" is "elastic", influenced by "supply", "consumption", and "growth" - signs of a free market. We do see that "centralisation" is similar to "property" under Communism, but it's only one out of many words and we should refrain from hastening to a conclusion there.

```{r}
draw_word_cloud("taxes")
```

For "taxes", the similar words are inconclusive. We are unable to gather more information about the two schools' tendencies regarding "taxes".

```{r}
draw_word_cloud("poor")
```

Under Communism, the words similar to "poor" are mild, describing a poor person ("tenant", "poverty", "miserable"). However, Capitalism seems to be harsh in its opinion of "poor". Words such as "unproductive", "prodigal", and "servants" seem to link being "poor" to being inadequate in terms of wealth management and decision making. 

Opinion: This may suggest that lack of money is not respected in this framework, even though it might be this framework itself leading some people to this situation.

```{r}
draw_word_cloud("commerce")
```

We can not gather enough information from words similar to "commerce".

# Discussion

## Data-Driven Results

So. 

Here's what we know about **Communist** texts purely from exploring the data.

+ Use a lot of words like "labour", "production", and "social" - a combination of economical and societal ideas.
+ Authors tend to write more about the topics of "Socieconomics", "Production", and "Time".
+ Have "communal" views of "property". 
+ Have an un-opinionated view of "wages"

And here's what we know about **Capitalist** texts purely from exploring the data.

+ Use a lot of words like "labour", "money", and "land" - mainly economical asset-related words.
+ Authors tend to write more about the topics "Finance & Banking", "property", "Trade", "Politics", and "Assets".
+ Have "person"al views of "property"
+ Use the word "wages" closely with "profits".
+ Tend to use "poor" alongside "unproductive" and "prodigal".
    
## Background Knowledge

In reality, we actually know a lot about the ideals of Communism and Capitalism. 

1. Communism
    + has state-owned property
    + has no free market
    + distributes wealth equally
    + does not have social classes
    + focuses of the whole community's welfare
    
2. Capitalism

    + allows personal ownership of property
    + operates on a free market
    + supposedly distributes wealth based on a measure of "effort" - unequal distribution
    + has class distinctions
    + has an individualistic view of welfare
    
# Conclusion

So, it turns out that the data spoke correctly! Even when we were careful to avoid confirmation bias, the data seemed to support a lot of the truth about Communism and Capitalism. This goes on to demonstrate the power of data science and natural language processing.

